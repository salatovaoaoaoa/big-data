{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bccd10-9219-459f-88e3-de1f13a8f049",
   "metadata": {},
   "source": [
    "## Устанавливаем библиотеку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d6fc82-4235-42fd-9e9e-924489475128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mrjob\n",
      "  Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.6/439.6 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from mrjob) (6.0.1)\n",
      "Installing collected packages: mrjob\n",
      "Successfully installed mrjob-0.7.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead684e-eb6b-4364-b673-ed9d3489cb18",
   "metadata": {},
   "source": [
    "## Пишем код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c65fb03-4ab4-44b8-a308-d935d7fb4c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount.py\n",
    "# %%file is an Ipython magic function that saves the code cell as a file\n",
    "\n",
    "from mrjob.job import MRJob # import the mrjob library\n",
    "\n",
    "class MRSongCount(MRJob):\n",
    "    #В маппер приходят отдельные строки\n",
    "    #Например, one, two, three\n",
    "    #Например, one, two, five\n",
    "    #Например, two, three, four\n",
    "    def mapper(self, _, line):\n",
    "        words = line.lower().replace(\"(\", '').replace(\")\", '').replace(\"!\", '').replace(\".\", '').replace(\",\", '').split()\n",
    "\n",
    "        word_map = {}\n",
    "        for word in words:\n",
    "            if word in word_map:\n",
    "                word_map[word] = word_map[word] + 1\n",
    "            else:\n",
    "                word_map[word] = 1\n",
    "\n",
    "        for word,size in word_map.items():\n",
    "            yield (word, size)\n",
    "    #На выходе у нас пары: (key, value)\n",
    "    # (one, 1)\n",
    "    # (two, 1)\n",
    "    # (three, 1)\n",
    "    # и так далее\n",
    "\n",
    "    \n",
    "    #На выходе у нас пары: (key, спислк всех значений, которые мы получили на предыдущей стадии)\n",
    "    # (one, [1,1])\n",
    "    # (two, [1,1, 1])\n",
    "    # (three, [1,1])\n",
    "    # (four, [1])\n",
    "    # и так далее\n",
    "    def reducer(self, word, values):\n",
    "        yield (word, sum(values))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRSongCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ec9d9-2a9b-4a14-99a5-82532c4e3908",
   "metadata": {},
   "source": [
    "## Протестируем локально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "952acbad-6f0b-49b7-bac0-e7dd204faeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/wordcount.root.20231107.173223.699763\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/wordcount.root.20231107.173223.699763/output\n",
      "Streaming final output from /tmp/wordcount.root.20231107.173223.699763/output...\n",
      "\"i'm\"\t1\n",
      "\"in\"\t4\n",
      "\"inside\"\t12\n",
      "\"is\"\t4\n",
      "\"isn't\"\t4\n",
      "\"it\"\t1\n",
      "\"it's\"\t2\n",
      "\"let\"\t12\n",
      "\"she's\"\t4\n",
      "\"slave\"\t1\n",
      "\"so\"\t2\n",
      "\"solemn\"\t1\n",
      "\"something\"\t1\n",
      "\"stress\"\t1\n",
      "\"such\"\t1\n",
      "\"temples\"\t1\n",
      "\"terrorize\"\t1\n",
      "\"that\"\t5\n",
      "\"to\"\t6\n",
      "\"unchecked\"\t1\n",
      "\"up\"\t12\n",
      "\"vixen\"\t1\n",
      "\"what\"\t1\n",
      "\"when\"\t1\n",
      "\"oh\"\t2\n",
      "\"one\"\t2\n",
      "\"only\"\t2\n",
      "\"or\"\t1\n",
      "\"past\"\t1\n",
      "\"perverse\"\t1\n",
      "\"pheromone\"\t1\n",
      "\"possession\"\t1\n",
      "\"press\"\t1\n",
      "\"real\"\t8\n",
      "\"won't\"\t12\n",
      "\"worse\"\t1\n",
      "\"yeah\"\t1\n",
      "\"yet\"\t1\n",
      "\"but\"\t1\n",
      "\"came\"\t1\n",
      "\"can't\"\t4\n",
      "\"carve\"\t1\n",
      "\"caught\"\t1\n",
      "\"chest\"\t1\n",
      "\"climatic\"\t1\n",
      "\"collectors\"\t1\n",
      "\"coming\"\t3\n",
      "\"continues\"\t1\n",
      "\"crazy\"\t1\n",
      "\"recognize\"\t1\n",
      "\"restraints\"\t1\n",
      "\"rings\"\t1\n",
      "\"sad\"\t2\n",
      "\"say\"\t1\n",
      "\"see\"\t2\n",
      "\"seems\"\t1\n",
      "\"self-oblige\"\t1\n",
      "\"she\"\t10\n",
      "\"more\"\t1\n",
      "\"my\"\t6\n",
      "\"name\"\t1\n",
      "\"need\"\t1\n",
      "\"nervous\"\t1\n",
      "\"never\"\t1\n",
      "\"night\"\t1\n",
      "\"no\"\t1\n",
      "\"now\"\t1\n",
      "\"of\"\t13\n",
      "\"make\"\t4\n",
      "\"makes\"\t2\n",
      "\"master\"\t1\n",
      "\"me\"\t16\n",
      "\"cult\"\t1\n",
      "\"dahlia\"\t1\n",
      "\"despise\"\t1\n",
      "\"devious\"\t1\n",
      "\"dressed\"\t1\n",
      "\"enter\"\t1\n",
      "\"everything\"\t1\n",
      "\"exist\"\t1\n",
      "\"face\"\t1\n",
      "\"fatalaties\"\t1\n",
      "\"forever\"\t2\n",
      "\"fragile\"\t1\n",
      "\"get\"\t1\n",
      "\"hands\"\t1\n",
      "\"the\"\t6\n",
      "\"this\"\t12\n",
      "\"through\"\t1\n",
      "\"a\"\t3\n",
      "\"alive\"\t2\n",
      "\"all\"\t1\n",
      "\"am\"\t1\n",
      "\"and\"\t4\n",
      "\"aphid\"\t1\n",
      "\"astounding\"\t1\n",
      "\"attention\"\t1\n",
      "\"attraction\"\t1\n",
      "\"bathed\"\t1\n",
      "\"build\"\t12\n",
      "\"hard\"\t1\n",
      "\"her\"\t6\n",
      "\"home\"\t3\n",
      "\"hypnotic\"\t1\n",
      "\"i\"\t21\n",
      "Removing temp directory /tmp/wordcount.root.20231107.173223.699763...\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcount.py input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde09257-4c77-4d71-8e60-d93e88342045",
   "metadata": {},
   "source": [
    "## Запустим на кластере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a21f5698-6bef-4ad5-b046-dfaeb36449fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/wordcount.root.20231107.173246.843147\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.173246.843147/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.173246.843147/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar3012102092679687138/] [] /tmp/streamjob10416955896905894053.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1699356031669_0007\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1699356031669_0007\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1699356031669_0007\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1699356031669_0007/\n",
      "  Running job: job_1699356031669_0007\n",
      "  Job job_1699356031669_0007 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1699356031669_0007 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.173246.843147/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2183\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1066\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3151\n",
      "\t\tFILE: Number of bytes written=853528\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2363\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=1066\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5742592\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2429952\n",
      "\t\tTotal time spent by all map tasks (ms)=5608\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11216\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2373\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4746\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5608\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2373\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1880\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=57\n",
      "\t\tInput split bytes=180\n",
      "\t\tMap input records=50\n",
      "\t\tMap output bytes=2567\n",
      "\t\tMap output materialized bytes=3157\n",
      "\t\tMap output records=289\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=265764864\n",
      "\t\tPeak Map Virtual memory (bytes)=2742087680\n",
      "\t\tPeak Reduce Physical memory (bytes)=225984512\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2743996416\n",
      "\t\tPhysical memory (bytes) snapshot=755585024\n",
      "\t\tReduce input groups=105\n",
      "\t\tReduce input records=289\n",
      "\t\tReduce output records=105\n",
      "\t\tReduce shuffle bytes=3157\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=578\n",
      "\t\tTotal committed heap usage (bytes)=505413632\n",
      "\t\tVirtual memory (bytes) snapshot=8218787840\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.173246.843147/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.173246.843147/output...\n",
      "\"a\"\t3\n",
      "\"alive\"\t2\n",
      "\"all\"\t1\n",
      "\"am\"\t1\n",
      "\"and\"\t4\n",
      "\"aphid\"\t1\n",
      "\"astounding\"\t1\n",
      "\"attention\"\t1\n",
      "\"attraction\"\t1\n",
      "\"bathed\"\t1\n",
      "\"build\"\t12\n",
      "\"but\"\t1\n",
      "\"came\"\t1\n",
      "\"can't\"\t4\n",
      "\"carve\"\t1\n",
      "\"caught\"\t1\n",
      "\"chest\"\t1\n",
      "\"climatic\"\t1\n",
      "\"collectors\"\t1\n",
      "\"coming\"\t3\n",
      "\"continues\"\t1\n",
      "\"crazy\"\t1\n",
      "\"cult\"\t1\n",
      "\"dahlia\"\t1\n",
      "\"despise\"\t1\n",
      "\"devious\"\t1\n",
      "\"dressed\"\t1\n",
      "\"enter\"\t1\n",
      "\"everything\"\t1\n",
      "\"exist\"\t1\n",
      "\"face\"\t1\n",
      "\"fatalaties\"\t1\n",
      "\"forever\"\t2\n",
      "\"fragile\"\t1\n",
      "\"get\"\t1\n",
      "\"hands\"\t1\n",
      "\"hard\"\t1\n",
      "\"her\"\t6\n",
      "\"home\"\t3\n",
      "\"hypnotic\"\t1\n",
      "\"i\"\t21\n",
      "\"i'm\"\t1\n",
      "\"in\"\t4\n",
      "\"inside\"\t12\n",
      "\"is\"\t4\n",
      "\"isn't\"\t4\n",
      "\"it\"\t1\n",
      "\"it's\"\t2\n",
      "\"let\"\t12\n",
      "\"make\"\t4\n",
      "\"makes\"\t2\n",
      "\"master\"\t1\n",
      "\"me\"\t16\n",
      "\"more\"\t1\n",
      "\"my\"\t6\n",
      "\"name\"\t1\n",
      "\"need\"\t1\n",
      "\"nervous\"\t1\n",
      "\"never\"\t1\n",
      "\"night\"\t1\n",
      "\"no\"\t1\n",
      "\"now\"\t1\n",
      "\"of\"\t13\n",
      "\"oh\"\t2\n",
      "\"one\"\t2\n",
      "\"only\"\t2\n",
      "\"or\"\t1\n",
      "\"past\"\t1\n",
      "\"perverse\"\t1\n",
      "\"pheromone\"\t1\n",
      "\"possession\"\t1\n",
      "\"press\"\t1\n",
      "\"real\"\t8\n",
      "\"recognize\"\t1\n",
      "\"restraints\"\t1\n",
      "\"rings\"\t1\n",
      "\"sad\"\t2\n",
      "\"say\"\t1\n",
      "\"see\"\t2\n",
      "\"seems\"\t1\n",
      "\"self-oblige\"\t1\n",
      "\"she\"\t10\n",
      "\"she's\"\t4\n",
      "\"slave\"\t1\n",
      "\"so\"\t2\n",
      "\"solemn\"\t1\n",
      "\"something\"\t1\n",
      "\"stress\"\t1\n",
      "\"such\"\t1\n",
      "\"temples\"\t1\n",
      "\"terrorize\"\t1\n",
      "\"that\"\t5\n",
      "\"the\"\t6\n",
      "\"this\"\t12\n",
      "\"through\"\t1\n",
      "\"to\"\t6\n",
      "\"unchecked\"\t1\n",
      "\"up\"\t12\n",
      "\"vixen\"\t1\n",
      "\"what\"\t1\n",
      "\"when\"\t1\n",
      "\"won't\"\t12\n",
      "\"worse\"\t1\n",
      "\"yeah\"\t1\n",
      "\"yet\"\t1\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.173246.843147...\n",
      "Removing temp directory /tmp/wordcount.root.20231107.173246.843147...\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcount.py -r hadoop hdfs://namenode:8020/pyarrow/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0dff5-29f0-4ffe-a3a0-9858b107e842",
   "metadata": {},
   "source": [
    "## Попробуем увеличить число reducer-ов. Для этого запишем настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "86b2ac0b-ee11-4d2e-be48-19fcd5cb2c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.conf\n"
     ]
    }
   ],
   "source": [
    "%%file config.conf\n",
    "\n",
    "runners:\n",
    "  hadoop: # also works for emr runner\n",
    "    jobconf:\n",
    "      mapreduce.job.reduces: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dd207f61-d269-4194-9b27-c1caf30358ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/wordcount.root.20231107.192558.879583\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.192558.879583/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.192558.879583/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar18357820264371207698/] [] /tmp/streamjob8957403416951307493.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1699384953884_0003\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1699384953884_0003\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1699384953884_0003\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1699384953884_0003/\n",
      "  Running job: job_1699384953884_0003\n",
      "  Job job_1699384953884_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1699384953884_0003 completed successfully\n",
      "  Output directory: hdfs:///pyarrow/output2\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2183\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1066\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3157\n",
      "\t\tFILE: Number of bytes written=1135828\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2363\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=1066\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5455872\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5622784\n",
      "\t\tTotal time spent by all map tasks (ms)=5328\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10656\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5491\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10982\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5328\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5491\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2850\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=66\n",
      "\t\tInput split bytes=180\n",
      "\t\tMap input records=50\n",
      "\t\tMap output bytes=2567\n",
      "\t\tMap output materialized bytes=3169\n",
      "\t\tMap output records=289\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPeak Map Physical memory (bytes)=273022976\n",
      "\t\tPeak Map Virtual memory (bytes)=2746404864\n",
      "\t\tPeak Reduce Physical memory (bytes)=221908992\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2740150272\n",
      "\t\tPhysical memory (bytes) snapshot=975007744\n",
      "\t\tReduce input groups=105\n",
      "\t\tReduce input records=289\n",
      "\t\tReduce output records=105\n",
      "\t\tReduce shuffle bytes=3169\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=578\n",
      "\t\tTotal committed heap usage (bytes)=637534208\n",
      "\t\tVirtual memory (bytes) snapshot=10966384640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///pyarrow/output2\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.192558.879583...\n",
      "Removing temp directory /tmp/wordcount.root.20231107.192558.879583...\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcount.py -r hadoop hdfs://namenode:8020/pyarrow/input.txt --conf-path config.conf --output /pyarrow/output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c19ea2fe-3480-448c-9beb-71f05915e0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"am\"\t1\n",
      "\"aphid\"\t1\n",
      "\"astounding\"\t1\n",
      "\"attention\"\t1\n",
      "\"bathed\"\t1\n",
      "\"build\"\t12\n",
      "\"came\"\t1\n",
      "\"caught\"\t1\n",
      "\"climatic\"\t1\n",
      "\"collectors\"\t1\n",
      "\"continues\"\t1\n",
      "\"cult\"\t1\n",
      "\"dressed\"\t1\n",
      "\"enter\"\t1\n",
      "\"fatalaties\"\t1\n",
      "\"fragile\"\t1\n",
      "\"get\"\t1\n",
      "\"hands\"\t1\n",
      "\"hypnotic\"\t1\n",
      "\"inside\"\t12\n",
      "\"is\"\t4\n",
      "\"make\"\t4\n",
      "\"master\"\t1\n",
      "\"me\"\t16\n",
      "\"my\"\t6\n",
      "\"need\"\t1\n",
      "\"nervous\"\t1\n",
      "\"never\"\t1\n",
      "\"night\"\t1\n",
      "\"now\"\t1\n",
      "\"one\"\t2\n",
      "\"only\"\t2\n",
      "\"past\"\t1\n",
      "\"perverse\"\t1\n",
      "\"possession\"\t1\n",
      "\"real\"\t8\n",
      "\"recognize\"\t1\n",
      "\"sad\"\t2\n",
      "\"she\"\t10\n",
      "\"she's\"\t4\n",
      "\"so\"\t2\n",
      "\"solemn\"\t1\n",
      "\"something\"\t1\n",
      "\"stress\"\t1\n",
      "\"temples\"\t1\n",
      "\"terrorize\"\t1\n",
      "\"this\"\t12\n",
      "\"unchecked\"\t1\n",
      "\"vixen\"\t1\n",
      "\"what\"\t1\n",
      "\"when\"\t1\n",
      "\"worse\"\t1\n",
      "\"yet\"\t1\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /pyarrow/output2/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e2f84-21dd-441c-b84f-f330f1d61a8b",
   "metadata": {},
   "source": [
    "## Добавляем combine стадию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b2d45b-24e3-48e1-92c3-f1733063254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount.py\n",
    "# %%file is an Ipython magic function that saves the code cell as a file\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # optimization: sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostUsedWord.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d66e4ab0-c172-446c-9b3b-9987779358d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/wordcount.root.20231107.193931.170381\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.193931.170381/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.193931.170381/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2998626655543088211/] [] /tmp/streamjob15024210008432738021.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1699384953884_0005\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1699384953884_0005\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1699384953884_0005\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1699384953884_0005/\n",
      "  Running job: job_1699384953884_0005\n",
      "  Job job_1699384953884_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1699384953884_0005 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.193931.170381/step-output/0000\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2183\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1918\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1471\n",
      "\t\tFILE: Number of bytes written=1134408\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2363\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=1918\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6255616\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5538816\n",
      "\t\tTotal time spent by all map tasks (ms)=6109\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12218\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5409\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10818\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6109\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5409\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3400\n",
      "\t\tCombine input records=296\n",
      "\t\tCombine output records=124\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=52\n",
      "\t\tInput split bytes=180\n",
      "\t\tMap input records=50\n",
      "\t\tMap output bytes=2618\n",
      "\t\tMap output materialized bytes=1483\n",
      "\t\tMap output records=296\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPeak Map Physical memory (bytes)=278892544\n",
      "\t\tPeak Map Virtual memory (bytes)=2744799232\n",
      "\t\tPeak Reduce Physical memory (bytes)=224092160\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2749194240\n",
      "\t\tPhysical memory (bytes) snapshot=996868096\n",
      "\t\tReduce input groups=106\n",
      "\t\tReduce input records=124\n",
      "\t\tReduce output records=106\n",
      "\t\tReduce shuffle bytes=1483\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=248\n",
      "\t\tTotal committed heap usage (bytes)=740294656\n",
      "\t\tVirtual memory (bytes) snapshot=10979815424\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar13510210756386612822/] [] /tmp/streamjob12514543563146605839.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1699384953884_0006\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1699384953884_0006\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1699384953884_0006\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1699384953884_0006/\n",
      "  Running job: job_1699384953884_0006\n",
      "  Job job_1699384953884_0006 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1699384953884_0006 completed successfully\n",
      "  Output directory: hdfs:///pyarrow/output2\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1918\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2142\n",
      "\t\tFILE: Number of bytes written=1133674\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2224\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=7\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5747712\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=6194176\n",
      "\t\tTotal time spent by all map tasks (ms)=5613\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11226\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6049\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12098\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5613\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6049\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2670\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=74\n",
      "\t\tInput split bytes=306\n",
      "\t\tMap input records=106\n",
      "\t\tMap output bytes=1918\n",
      "\t\tMap output materialized bytes=2154\n",
      "\t\tMap output records=106\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tPeak Map Physical memory (bytes)=275451904\n",
      "\t\tPeak Map Virtual memory (bytes)=2740297728\n",
      "\t\tPeak Reduce Physical memory (bytes)=216178688\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2745622528\n",
      "\t\tPhysical memory (bytes) snapshot=964689920\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=106\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=2154\n",
      "\t\tShuffled Maps =4\n",
      "\t\tSpilled Records=212\n",
      "\t\tTotal committed heap usage (bytes)=740294656\n",
      "\t\tVirtual memory (bytes) snapshot=10960117760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///pyarrow/output2\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.193931.170381...\n",
      "Removing temp directory /tmp/wordcount.root.20231107.193931.170381...\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcount.py -r hadoop hdfs://namenode:8020/pyarrow/input.txt --conf-path config.conf --output /pyarrow/output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e81d9534-1735-4e5f-80f1-94bf2cc0647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -cat /pyarrow/output2/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e1b8c-256b-41b7-9d7c-2a252b7ac844",
   "metadata": {},
   "source": [
    "## Попробуем использовать каунтер. В рамках маппера мы можем к нему обращаться, но на редьюсере мы его уже не видим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1181ec4-a4c5-4114-a139-9f8904fa0b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount.py\n",
    "# %%file is an Ipython magic function that saves the code cell as a file\n",
    "\n",
    "from mrjob.job import MRJob # import the mrjob library\n",
    "\n",
    "class MRSongCount(MRJob):\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "        super().__init__()\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        words = line.lower().replace(\"(\", '').replace(\")\", '').replace(\"!\", '').split()\n",
    "\n",
    "        word_map = {}\n",
    "        for word in words:\n",
    "            self.counter = self.counter + 1\n",
    "            if word in word_map:\n",
    "                word_map[word] = word_map[word] + 1\n",
    "            else:\n",
    "                word_map[word] = 1\n",
    "\n",
    "        for word,size in word_map.items():                \n",
    "            yield (word, size)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield (key, f\"{sum(values)} of {self.counter}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRSongCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3de9013-9b16-47f9-831f-a4c32116c60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/wordcount.root.20231106.122525.608203\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/wordcount.root.20231106.122525.608203/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/wordcount.root.20231106.122525.608203/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6103127981734022064/] [] /tmp/streamjob14595565506979629039.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1699271312516_0003\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1699271312516_0003\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1699271312516_0003\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1699271312516_0003/\n",
      "  Running job: job_1699271312516_0003\n",
      "  Job job_1699271312516_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1699271312516_0003 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/wordcount.root.20231106.122525.608203/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2183\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1839\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3170\n",
      "\t\tFILE: Number of bytes written=852435\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2363\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=1839\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10016768\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2554880\n",
      "\t\tTotal time spent by all map tasks (ms)=9782\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19564\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2495\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4990\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9782\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2495\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2300\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=50\n",
      "\t\tInput split bytes=180\n",
      "\t\tMap input records=50\n",
      "\t\tMap output bytes=2584\n",
      "\t\tMap output materialized bytes=3176\n",
      "\t\tMap output records=290\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=276316160\n",
      "\t\tPeak Map Virtual memory (bytes)=2747060224\n",
      "\t\tPeak Reduce Physical memory (bytes)=219672576\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2743660544\n",
      "\t\tPhysical memory (bytes) snapshot=771092480\n",
      "\t\tReduce input groups=107\n",
      "\t\tReduce input records=290\n",
      "\t\tReduce output records=107\n",
      "\t\tReduce shuffle bytes=3176\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=580\n",
      "\t\tTotal committed heap usage (bytes)=506462208\n",
      "\t\tVirtual memory (bytes) snapshot=8237707264\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/wordcount.root.20231106.122525.608203/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/wordcount.root.20231106.122525.608203/output...\n",
      "\"a\"\t\"3 of 0\"\n",
      "\"alive\"\t\"1 of 0\"\n",
      "\"alive,\"\t\"1 of 0\"\n",
      "\"all\"\t\"1 of 0\"\n",
      "\"am\"\t\"1 of 0\"\n",
      "\"and\"\t\"4 of 0\"\n",
      "\"aphid\"\t\"1 of 0\"\n",
      "\"astounding\"\t\"1 of 0\"\n",
      "\"attention\"\t\"1 of 0\"\n",
      "\"attraction\"\t\"1 of 0\"\n",
      "\"bathed\"\t\"1 of 0\"\n",
      "\"build\"\t\"12 of 0\"\n",
      "\"but\"\t\"1 of 0\"\n",
      "\"came\"\t\"1 of 0\"\n",
      "\"can't\"\t\"4 of 0\"\n",
      "\"carve\"\t\"1 of 0\"\n",
      "\"caught\"\t\"1 of 0\"\n",
      "\"chest\"\t\"1 of 0\"\n",
      "\"climatic\"\t\"1 of 0\"\n",
      "\"collectors\"\t\"1 of 0\"\n",
      "\"coming\"\t\"3 of 0\"\n",
      "\"continues\"\t\"1 of 0\"\n",
      "\"crazy,\"\t\"1 of 0\"\n",
      "\"cult\"\t\"1 of 0\"\n",
      "\"dahlia\"\t\"1 of 0\"\n",
      "\"despise\"\t\"1 of 0\"\n",
      "\"devious\"\t\"1 of 0\"\n",
      "\"dressed\"\t\"1 of 0\"\n",
      "\"enter\"\t\"1 of 0\"\n",
      "\"everything\"\t\"1 of 0\"\n",
      "\"exist\"\t\"1 of 0\"\n",
      "\"face\"\t\"1 of 0\"\n",
      "\"fatalaties\"\t\"1 of 0\"\n",
      "\"forever\"\t\"2 of 0\"\n",
      "\"fragile\"\t\"1 of 0\"\n",
      "\"get\"\t\"1 of 0\"\n",
      "\"hands\"\t\"1 of 0\"\n",
      "\"hard\"\t\"1 of 0\"\n",
      "\"her\"\t\"6 of 0\"\n",
      "\"home\"\t\"3 of 0\"\n",
      "\"hypnotic\"\t\"1 of 0\"\n",
      "\"i\"\t\"21 of 0\"\n",
      "\"i'm\"\t\"1 of 0\"\n",
      "\"in\"\t\"4 of 0\"\n",
      "\"inside\"\t\"12 of 0\"\n",
      "\"is\"\t\"4 of 0\"\n",
      "\"isn't\"\t\"4 of 0\"\n",
      "\"it\"\t\"1 of 0\"\n",
      "\"it's\"\t\"2 of 0\"\n",
      "\"let\"\t\"12 of 0\"\n",
      "\"make\"\t\"4 of 0\"\n",
      "\"makes\"\t\"2 of 0\"\n",
      "\"master\"\t\"1 of 0\"\n",
      "\"me\"\t\"15 of 0\"\n",
      "\"me,\"\t\"1 of 0\"\n",
      "\"more\"\t\"1 of 0\"\n",
      "\"my\"\t\"6 of 0\"\n",
      "\"name\"\t\"1 of 0\"\n",
      "\"need,\"\t\"1 of 0\"\n",
      "\"nervous,\"\t\"1 of 0\"\n",
      "\"never\"\t\"1 of 0\"\n",
      "\"night\"\t\"1 of 0\"\n",
      "\"no\"\t\"1 of 0\"\n",
      "\"now\"\t\"1 of 0\"\n",
      "\"of\"\t\"13 of 0\"\n",
      "\"oh\"\t\"2 of 0\"\n",
      "\"one\"\t\"2 of 0\"\n",
      "\"only\"\t\"2 of 0\"\n",
      "\"or\"\t\"1 of 0\"\n",
      "\"past\"\t\"1 of 0\"\n",
      "\"perverse,\"\t\"1 of 0\"\n",
      "\"pheromone\"\t\"1 of 0\"\n",
      "\"possession\"\t\"1 of 0\"\n",
      "\"press\"\t\"1 of 0\"\n",
      "\"real\"\t\"8 of 0\"\n",
      "\"recognize\"\t\"1 of 0\"\n",
      "\"restraints\"\t\"1 of 0\"\n",
      "\"rings\"\t\"1 of 0\"\n",
      "\"sad\"\t\"2 of 0\"\n",
      "\"say\"\t\"1 of 0\"\n",
      "\"see\"\t\"2 of 0\"\n",
      "\"seems\"\t\"1 of 0\"\n",
      "\"self-oblige\"\t\"1 of 0\"\n",
      "\"she\"\t\"10 of 0\"\n",
      "\"she's\"\t\"4 of 0\"\n",
      "\"slave,\"\t\"1 of 0\"\n",
      "\"so\"\t\"2 of 0\"\n",
      "\"solemn\"\t\"1 of 0\"\n",
      "\"something\"\t\"1 of 0\"\n",
      "\"stress\"\t\"1 of 0\"\n",
      "\"such\"\t\"1 of 0\"\n",
      "\"temples\"\t\"1 of 0\"\n",
      "\"terrorize\"\t\"1 of 0\"\n",
      "\"that\"\t\"5 of 0\"\n",
      "\"the\"\t\"6 of 0\"\n",
      "\"this\"\t\"12 of 0\"\n",
      "\"through\"\t\"1 of 0\"\n",
      "\"to\"\t\"6 of 0\"\n",
      "\"unchecked\"\t\"1 of 0\"\n",
      "\"up\"\t\"12 of 0\"\n",
      "\"vixen\"\t\"1 of 0\"\n",
      "\"what\"\t\"1 of 0\"\n",
      "\"when\"\t\"1 of 0\"\n",
      "\"won't\"\t\"12 of 0\"\n",
      "\"worse\"\t\"1 of 0\"\n",
      "\"yeah\"\t\"1 of 0\"\n",
      "\"yet\"\t\"1 of 0\"\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/wordcount.root.20231106.122525.608203...\n",
      "Removing temp directory /tmp/wordcount.root.20231106.122525.608203...\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcount.py -r hadoop hdfs://namenode:8020/pyarrow/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191be5be-4736-4f8f-bac8-1616f7acf2ec",
   "metadata": {},
   "source": [
    "## Конфиг для запуска в определенной очереди"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c86dd26d-ac66-4430-8c2f-cdd92d62ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.conf\n"
     ]
    }
   ],
   "source": [
    "%%file config.conf\n",
    "\n",
    "runners:\n",
    "  hadoop: # also works for emr runner\n",
    "    jobconf:\n",
    "      mapreduce.job.queuename: jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1c364ef5-8783-4cdd-a74a-ca06880c4c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/wordcount.root.20231107.192343.326396\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.192343.326396/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.192343.326396/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar13723236156834957118/] [] /tmp/streamjob11680448707763552907.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.2:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1699384953884_0001\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1699384953884_0001\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1699384953884_0001\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1699384953884_0001/\n",
      "  Running job: job_1699384953884_0001\n",
      "  Job job_1699384953884_0001 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1699384953884_0001 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.192343.326396/output\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2183\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1066\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3151\n",
      "\t\tFILE: Number of bytes written=853579\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2363\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=1066\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5728256\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2434048\n",
      "\t\tTotal time spent by all map tasks (ms)=5594\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11188\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2377\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4754\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5594\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2377\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2090\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=58\n",
      "\t\tInput split bytes=180\n",
      "\t\tMap input records=50\n",
      "\t\tMap output bytes=2567\n",
      "\t\tMap output materialized bytes=3157\n",
      "\t\tMap output records=289\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=276803584\n",
      "\t\tPeak Map Virtual memory (bytes)=2745753600\n",
      "\t\tPeak Reduce Physical memory (bytes)=225169408\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2741866496\n",
      "\t\tPhysical memory (bytes) snapshot=773439488\n",
      "\t\tReduce input groups=105\n",
      "\t\tReduce input records=289\n",
      "\t\tReduce output records=105\n",
      "\t\tReduce shuffle bytes=3157\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=578\n",
      "\t\tTotal committed heap usage (bytes)=608174080\n",
      "\t\tVirtual memory (bytes) snapshot=8231071744\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.192343.326396/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.192343.326396/output...\n",
      "\"a\"\t3\n",
      "\"alive\"\t2\n",
      "\"all\"\t1\n",
      "\"am\"\t1\n",
      "\"and\"\t4\n",
      "\"aphid\"\t1\n",
      "\"astounding\"\t1\n",
      "\"attention\"\t1\n",
      "\"attraction\"\t1\n",
      "\"bathed\"\t1\n",
      "\"build\"\t12\n",
      "\"but\"\t1\n",
      "\"came\"\t1\n",
      "\"can't\"\t4\n",
      "\"carve\"\t1\n",
      "\"caught\"\t1\n",
      "\"chest\"\t1\n",
      "\"climatic\"\t1\n",
      "\"collectors\"\t1\n",
      "\"coming\"\t3\n",
      "\"continues\"\t1\n",
      "\"crazy\"\t1\n",
      "\"cult\"\t1\n",
      "\"dahlia\"\t1\n",
      "\"despise\"\t1\n",
      "\"devious\"\t1\n",
      "\"dressed\"\t1\n",
      "\"enter\"\t1\n",
      "\"everything\"\t1\n",
      "\"exist\"\t1\n",
      "\"face\"\t1\n",
      "\"fatalaties\"\t1\n",
      "\"forever\"\t2\n",
      "\"fragile\"\t1\n",
      "\"get\"\t1\n",
      "\"hands\"\t1\n",
      "\"hard\"\t1\n",
      "\"her\"\t6\n",
      "\"home\"\t3\n",
      "\"hypnotic\"\t1\n",
      "\"i\"\t21\n",
      "\"i'm\"\t1\n",
      "\"in\"\t4\n",
      "\"inside\"\t12\n",
      "\"is\"\t4\n",
      "\"isn't\"\t4\n",
      "\"it\"\t1\n",
      "\"it's\"\t2\n",
      "\"let\"\t12\n",
      "\"make\"\t4\n",
      "\"makes\"\t2\n",
      "\"master\"\t1\n",
      "\"me\"\t16\n",
      "\"more\"\t1\n",
      "\"my\"\t6\n",
      "\"name\"\t1\n",
      "\"need\"\t1\n",
      "\"nervous\"\t1\n",
      "\"never\"\t1\n",
      "\"night\"\t1\n",
      "\"no\"\t1\n",
      "\"now\"\t1\n",
      "\"of\"\t13\n",
      "\"oh\"\t2\n",
      "\"one\"\t2\n",
      "\"only\"\t2\n",
      "\"or\"\t1\n",
      "\"past\"\t1\n",
      "\"perverse\"\t1\n",
      "\"pheromone\"\t1\n",
      "\"possession\"\t1\n",
      "\"press\"\t1\n",
      "\"real\"\t8\n",
      "\"recognize\"\t1\n",
      "\"restraints\"\t1\n",
      "\"rings\"\t1\n",
      "\"sad\"\t2\n",
      "\"say\"\t1\n",
      "\"see\"\t2\n",
      "\"seems\"\t1\n",
      "\"self-oblige\"\t1\n",
      "\"she\"\t10\n",
      "\"she's\"\t4\n",
      "\"slave\"\t1\n",
      "\"so\"\t2\n",
      "\"solemn\"\t1\n",
      "\"something\"\t1\n",
      "\"stress\"\t1\n",
      "\"such\"\t1\n",
      "\"temples\"\t1\n",
      "\"terrorize\"\t1\n",
      "\"that\"\t5\n",
      "\"the\"\t6\n",
      "\"this\"\t12\n",
      "\"through\"\t1\n",
      "\"to\"\t6\n",
      "\"unchecked\"\t1\n",
      "\"up\"\t12\n",
      "\"vixen\"\t1\n",
      "\"what\"\t1\n",
      "\"when\"\t1\n",
      "\"won't\"\t12\n",
      "\"worse\"\t1\n",
      "\"yeah\"\t1\n",
      "\"yet\"\t1\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/wordcount.root.20231107.192343.326396...\n",
      "Removing temp directory /tmp/wordcount.root.20231107.192343.326396...\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcount.py -r hadoop hdfs://namenode:8020/pyarrow/input.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b264a38-8895-4443-b989-31e831db50e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

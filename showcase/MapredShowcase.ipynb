{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353da96a-4d2d-4d46-9216-fa2e58de58b0",
   "metadata": {},
   "source": [
    "## смотрим, какие либы у нас есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c875cffb-b33b-4bd0-96d7-77fb2a821025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop-mapreduce-client-app-3.3.6.jar\n",
      "hadoop-mapreduce-client-common-3.3.6.jar\n",
      "hadoop-mapreduce-client-core-3.3.6.jar\n",
      "hadoop-mapreduce-client-hs-3.3.6.jar\n",
      "hadoop-mapreduce-client-hs-plugins-3.3.6.jar\n",
      "hadoop-mapreduce-client-jobclient-3.3.6-tests.jar\n",
      "hadoop-mapreduce-client-jobclient-3.3.6.jar\n",
      "hadoop-mapreduce-client-nativetask-3.3.6.jar\n",
      "hadoop-mapreduce-client-shuffle-3.3.6.jar\n",
      "hadoop-mapreduce-client-uploader-3.3.6.jar\n",
      "hadoop-mapreduce-examples-3.3.6.jar\n",
      "jdiff\n",
      "lib-examples\n",
      "sources\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/hadoop/share/hadoop/mapreduce/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c4978-b48a-4236-a90f-cdee2386bbfe",
   "metadata": {},
   "source": [
    "## Просто зовем help у класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1b36dd-943a-48fc-9112-2cffe9e08058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example program must be given as the first argument.\n",
      "Valid program names are:\n",
      "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
      "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
      "  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
      "  dbcount: An example job that count the pageview counts from a database.\n",
      "  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
      "  grep: A map/reduce program that counts the matches of a regex in the input.\n",
      "  join: A job that effects a join over sorted, equally partitioned datasets\n",
      "  multifilewc: A job that counts words from several files.\n",
      "  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
      "  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
      "  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
      "  randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
      "  secondarysort: An example defining a secondary sort to the reduce.\n",
      "  sort: A map/reduce program that sorts the data written by the random writer.\n",
      "  sudoku: A sudoku solver.\n",
      "  teragen: Generate data for the terasort\n",
      "  terasort: Run the terasort\n",
      "  teravalidate: Checking results of terasort\n",
      "  wordcount: A map/reduce program that counts the words in the input files.\n",
      "  wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
      "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
      "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d8b7d-3ed0-410f-9891-596d3f0ce60a",
   "metadata": {},
   "source": [
    "## Просто зовем список параметров у класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb172d8a-35c5-486c-a0eb-6374ed390a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: wordcount <in> [<in>...] <out>\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c9c43-026a-42fc-bd88-99152bf23a69",
   "metadata": {},
   "source": [
    "## чистим аутпут, запускаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5342d44-c8e5-4187-a724-4b84cbac024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://namenode/output\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r hdfs://namenode/output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4e36a5-6b3e-49ae-8f2e-44bb4b7dc02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-07 20:00:31 INFO  DefaultNoHARMFailoverProxyProvider:64 - Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "2023-11-07 20:00:31 INFO  AHSProxy:43 - Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "2023-11-07 20:00:31 INFO  JobResourceUploader:907 - Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1699356031669_0002\n",
      "2023-11-07 20:00:32 INFO  FileInputFormat:300 - Total input files to process : 1\n",
      "2023-11-07 20:00:32 INFO  JobSubmitter:202 - number of splits:1\n",
      "2023-11-07 20:00:32 INFO  JobSubmitter:298 - Submitting tokens for job: job_1699356031669_0002\n",
      "2023-11-07 20:00:32 INFO  JobSubmitter:299 - Executing with tokens: []\n",
      "2023-11-07 20:00:32 INFO  Configuration:2854 - resource-types.xml not found\n",
      "2023-11-07 20:00:32 INFO  ResourceUtils:476 - Unable to find 'resource-types.xml'.\n",
      "2023-11-07 20:00:33 INFO  YarnClientImpl:338 - Submitted application application_1699356031669_0002\n",
      "2023-11-07 20:00:33 INFO  Job:1682 - The url to track the job: http://resourcemanager:8088/proxy/application_1699356031669_0002/\n",
      "2023-11-07 20:00:33 INFO  Job:1727 - Running job: job_1699356031669_0002\n",
      "2023-11-07 20:00:41 INFO  Job:1748 - Job job_1699356031669_0002 running in uber mode : false\n",
      "2023-11-07 20:00:41 INFO  Job:1755 -  map 0% reduce 0%\n",
      "2023-11-07 20:00:48 INFO  Job:1755 -  map 100% reduce 0%\n",
      "2023-11-07 20:00:53 INFO  Job:1755 -  map 100% reduce 100%\n",
      "2023-11-07 20:00:54 INFO  Job:1766 - Job job_1699356031669_0002 completed successfully\n",
      "2023-11-07 20:00:54 INFO  Job:1773 - Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1411\n",
      "\t\tFILE: Number of bytes written=558953\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1553\n",
      "\t\tHDFS: Number of bytes written=946\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8634\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5172\n",
      "\t\tTotal time spent by all map tasks (ms)=4317\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2586\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4317\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2586\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4420608\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2648064\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=50\n",
      "\t\tMap output records=295\n",
      "\t\tMap output bytes=2628\n",
      "\t\tMap output materialized bytes=1411\n",
      "\t\tInput split bytes=98\n",
      "\t\tCombine input records=295\n",
      "\t\tCombine output records=117\n",
      "\t\tReduce input groups=117\n",
      "\t\tReduce shuffle bytes=1411\n",
      "\t\tReduce input records=117\n",
      "\t\tReduce output records=117\n",
      "\t\tSpilled Records=234\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=49\n",
      "\t\tCPU time spent (ms)=1780\n",
      "\t\tPhysical memory (bytes) snapshot=469618688\n",
      "\t\tVirtual memory (bytes) snapshot=5481639936\n",
      "\t\tTotal committed heap usage (bytes)=414187520\n",
      "\t\tPeak Map Physical memory (bytes)=255197184\n",
      "\t\tPeak Map Virtual memory (bytes)=2736640000\n",
      "\t\tPeak Reduce Physical memory (bytes)=214810624\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2746269696\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1455\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=946\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /pyarrow/input.txt /output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16190725-b894-4d65-a415-1eef27a86928",
   "metadata": {},
   "source": [
    "## Настраиваем pyarrow, смотрим на результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20e5328-e0ed-4445-8ef3-fa5d7b839d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CLASSPATH'] = '/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar'\n",
    "\n",
    "from pyarrow import fs\n",
    "hdfs = fs.HadoopFileSystem(\"namenode\", 8020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "039c06b9-41eb-4695-934e-9940248f5cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Coming\t1\n",
      "(She's\t1\n",
      "(she's\t1\n",
      "But\t1\n",
      "Carve\t1\n",
      "Climatic\t1\n",
      "Dahlia\t1\n",
      "Enter\t1\n",
      "Forever\t2\n",
      "Hard\t1\n",
      "Her\t1\n",
      "I\t21\n",
      "I'm\t1\n",
      "It's\t1\n",
      "My\t1\n",
      "No\t1\n",
      "Of\t1\n",
      "Oh\t2\n",
      "She\t9\n",
      "So\t1\n",
      "Such\t1\n",
      "The\t1\n",
      "Vixen\t1\n",
      "Yeah!\t1\n",
      "a\t3\n",
      "alive)\t1\n",
      "alive,\t1\n",
      "all\t1\n",
      "am\t1\n",
      "and\t4\n",
      "aphid\t1\n",
      "astounding\t1\n",
      "attention\t1\n",
      "attraction\t1\n",
      "bathed\t1\n",
      "build\t12\n",
      "came\t1\n",
      "can't\t4\n",
      "caught\t1\n",
      "chest\t1\n",
      "collectors\t1\n",
      "coming\t2\n",
      "continues\t1\n",
      "crazy,\t1\n",
      "cult\t1\n",
      "despise\t1\n",
      "devious\t1\n",
      "dressed\t1\n",
      "everything\t1\n",
      "exist\t1\n",
      "face\t1\n",
      "fatalaties\t1\n",
      "fragile\t1\n",
      "get\t1\n",
      "hands\t1\n",
      "her\t5\n",
      "home\t3\n",
      "hypnotic\t1\n",
      "in\t4\n",
      "inside\t12\n",
      "is\t4\n",
      "isn't\t4\n",
      "it\t1\n",
      "it's\t1\n",
      "let\t12\n",
      "make\t4\n",
      "makes\t2\n",
      "master\t1\n",
      "me\t15\n",
      "me,\t1\n",
      "more\t1\n",
      "my\t5\n",
      "name\t1\n",
      "need,\t1\n",
      "nervous,\t1\n",
      "never\t1\n",
      "night\t1\n",
      "now\t1\n",
      "of\t12\n",
      "one\t2\n",
      "only\t2\n",
      "or\t1\n",
      "past\t1\n",
      "perverse,\t1\n",
      "pheromone\t1\n",
      "possession\t1\n",
      "press\t1\n",
      "real\t8\n",
      "recognize\t1\n",
      "restraints\t1\n",
      "rings\t1\n",
      "sad)\t2\n",
      "say\t1\n",
      "see\t2\n",
      "seems\t1\n",
      "self-oblige\t1\n",
      "she\t1\n",
      "she's\t2\n",
      "slave,\t1\n",
      "so\t1\n",
      "solemn\t1\n",
      "something\t1\n",
      "stress\t1\n",
      "temples\t1\n",
      "terrorize\t1\n",
      "that\t5\n",
      "the\t5\n",
      "this\t12\n",
      "through\t1\n",
      "to\t6\n",
      "unchecked\t1\n",
      "up\t12\n",
      "what\t1\n",
      "when\t1\n",
      "won't\t12\n",
      "worse\t1\n",
      "yet\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with hdfs.open_input_stream(\"/output/part-r-00000\") as file:\n",
    "    data = file.readall().decode(\"utf-8\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da55cfdf-e3a1-424b-b0e8-3745fc1901ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Coming 1\n",
      "(She's 1\n",
      "(she's 1\n",
      "But 1\n",
      "Carve 1\n",
      "Climatic 1\n",
      "Dahlia 1\n",
      "Enter 1\n",
      "Forever 2\n",
      "Hard 1\n",
      "Her 1\n",
      "I 21\n",
      "I'm 1\n",
      "It's 1\n",
      "My 1\n",
      "No 1\n",
      "Of 1\n",
      "Oh 2\n",
      "She 9\n",
      "So 1\n",
      "Such 1\n",
      "The 1\n",
      "Vixen 1\n",
      "Yeah! 1\n",
      "a 3\n",
      "alive) 1\n",
      "alive, 1\n",
      "all 1\n",
      "am 1\n",
      "and 4\n",
      "aphid 1\n",
      "astounding 1\n",
      "attention 1\n",
      "attraction 1\n",
      "bathed 1\n",
      "build 12\n",
      "came 1\n",
      "can't 4\n",
      "caught 1\n",
      "chest 1\n",
      "collectors 1\n",
      "coming 2\n",
      "continues 1\n",
      "crazy, 1\n",
      "cult 1\n",
      "despise 1\n",
      "devious 1\n",
      "dressed 1\n",
      "everything 1\n",
      "exist 1\n",
      "face 1\n",
      "fatalaties 1\n",
      "fragile 1\n",
      "get 1\n",
      "hands 1\n",
      "her 5\n",
      "home 3\n",
      "hypnotic 1\n",
      "in 4\n",
      "inside 12\n",
      "is 4\n",
      "isn't 4\n",
      "it 1\n",
      "it's 1\n",
      "let 12\n",
      "make 4\n",
      "makes 2\n",
      "master 1\n",
      "me 15\n",
      "me, 1\n",
      "more 1\n",
      "my 5\n",
      "name 1\n",
      "need, 1\n",
      "nervous, 1\n",
      "never 1\n",
      "night 1\n",
      "now 1\n",
      "of 12\n",
      "one 2\n",
      "only 2\n",
      "or 1\n",
      "past 1\n",
      "perverse, 1\n",
      "pheromone 1\n",
      "possession 1\n",
      "press 1\n",
      "real 8\n",
      "recognize 1\n",
      "restraints 1\n",
      "rings 1\n",
      "sad) 2\n",
      "say 1\n",
      "see 2\n",
      "seems 1\n",
      "self-oblige 1\n",
      "she 1\n",
      "she's 2\n",
      "slave, 1\n",
      "so 1\n",
      "solemn 1\n",
      "something 1\n",
      "stress 1\n",
      "temples 1\n",
      "terrorize 1\n",
      "that 5\n",
      "the 5\n",
      "this 12\n",
      "through 1\n",
      "to 6\n",
      "unchecked 1\n",
      "up 12\n",
      "what 1\n",
      "when 1\n",
      "won't 12\n",
      "worse 1\n",
      "yet 1\n",
      "295\n"
     ]
    }
   ],
   "source": [
    "size = 0\n",
    "for line in data.replace(\"\\t\", \" \").split(\"\\n\"):\n",
    "    if line == \"\":\n",
    "        continue\n",
    "    print(line)\n",
    "    size = size + int(line.split(\" \")[1])\n",
    "\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c1ad8f-85ff-4f3f-822f-5e515410a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Coming\t1\n",
      "(She's\t1\n",
      "(she's\t1\n",
      "But\t1\n",
      "Carve\t1\n",
      "Climatic\t1\n",
      "Dahlia\t1\n",
      "Enter\t1\n",
      "Forever\t2\n",
      "Hard\t1\n",
      "Her\t1\n",
      "I\t21\n",
      "I'm\t1\n",
      "It's\t1\n",
      "My\t1\n",
      "No\t1\n",
      "Of\t1\n",
      "Oh\t2\n",
      "She\t9\n",
      "So\t1\n",
      "Such\t1\n",
      "The\t1\n",
      "Vixen\t1\n",
      "Yeah!\t1\n",
      "a\t3\n",
      "alive)\t1\n",
      "alive,\t1\n",
      "all\t1\n",
      "am\t1\n",
      "and\t4\n",
      "aphid\t1\n",
      "astounding\t1\n",
      "attention\t1\n",
      "attraction\t1\n",
      "bathed\t1\n",
      "build\t12\n",
      "came\t1\n",
      "can't\t4\n",
      "caught\t1\n",
      "chest\t1\n",
      "collectors\t1\n",
      "coming\t2\n",
      "continues\t1\n",
      "crazy,\t1\n",
      "cult\t1\n",
      "despise\t1\n",
      "devious\t1\n",
      "dressed\t1\n",
      "everything\t1\n",
      "exist\t1\n",
      "face\t1\n",
      "fatalaties\t1\n",
      "fragile\t1\n",
      "get\t1\n",
      "hands\t1\n",
      "her\t5\n",
      "home\t3\n",
      "hypnotic\t1\n",
      "in\t4\n",
      "inside\t12\n",
      "is\t4\n",
      "isn't\t4\n",
      "it\t1\n",
      "it's\t1\n",
      "let\t12\n",
      "make\t4\n",
      "makes\t2\n",
      "master\t1\n",
      "me\t15\n",
      "me,\t1\n",
      "more\t1\n",
      "my\t5\n",
      "name\t1\n",
      "need,\t1\n",
      "nervous,\t1\n",
      "never\t1\n",
      "night\t1\n",
      "now\t1\n",
      "of\t12\n",
      "one\t2\n",
      "only\t2\n",
      "or\t1\n",
      "past\t1\n",
      "perverse,\t1\n",
      "pheromone\t1\n",
      "possession\t1\n",
      "press\t1\n",
      "real\t8\n",
      "recognize\t1\n",
      "restraints\t1\n",
      "rings\t1\n",
      "sad)\t2\n",
      "say\t1\n",
      "see\t2\n",
      "seems\t1\n",
      "self-oblige\t1\n",
      "she\t1\n",
      "she's\t2\n",
      "slave,\t1\n",
      "so\t1\n",
      "solemn\t1\n",
      "something\t1\n",
      "stress\t1\n",
      "temples\t1\n",
      "terrorize\t1\n",
      "that\t5\n",
      "the\t5\n",
      "this\t12\n",
      "through\t1\n",
      "to\t6\n",
      "unchecked\t1\n",
      "up\t12\n",
      "what\t1\n",
      "when\t1\n",
      "won't\t12\n",
      "worse\t1\n",
      "yet\t1\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4442be0-0995-409e-8c59-79d83e390e1c",
   "metadata": {},
   "source": [
    "## Запускаем пример подсчета числа Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d0d0798-87b3-45a2-824d-f4ce5c16adbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Maps  = 15\n",
      "Samples per Map = 1800\n",
      "Wrote input for Map #0\n",
      "Wrote input for Map #1\n",
      "Wrote input for Map #2\n",
      "Wrote input for Map #3\n",
      "Wrote input for Map #4\n",
      "Wrote input for Map #5\n",
      "Wrote input for Map #6\n",
      "Wrote input for Map #7\n",
      "Wrote input for Map #8\n",
      "Wrote input for Map #9\n",
      "Wrote input for Map #10\n",
      "Wrote input for Map #11\n",
      "Wrote input for Map #12\n",
      "Wrote input for Map #13\n",
      "Wrote input for Map #14\n",
      "Starting Job\n",
      "2023-11-07 20:09:27 INFO  DefaultNoHARMFailoverProxyProvider:64 - Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "2023-11-07 20:09:28 INFO  AHSProxy:43 - Connecting to Application History server at historyserver/172.18.0.7:10200\n",
      "2023-11-07 20:09:28 INFO  JobResourceUploader:907 - Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1699356031669_0005\n",
      "2023-11-07 20:09:28 INFO  FileInputFormat:300 - Total input files to process : 15\n",
      "2023-11-07 20:09:29 INFO  JobSubmitter:202 - number of splits:15\n",
      "2023-11-07 20:09:29 INFO  JobSubmitter:298 - Submitting tokens for job: job_1699356031669_0005\n",
      "2023-11-07 20:09:29 INFO  JobSubmitter:299 - Executing with tokens: []\n",
      "2023-11-07 20:09:29 INFO  Configuration:2854 - resource-types.xml not found\n",
      "2023-11-07 20:09:29 INFO  ResourceUtils:476 - Unable to find 'resource-types.xml'.\n",
      "2023-11-07 20:09:29 INFO  YarnClientImpl:338 - Submitted application application_1699356031669_0005\n",
      "2023-11-07 20:09:29 INFO  Job:1682 - The url to track the job: http://resourcemanager:8088/proxy/application_1699356031669_0005/\n",
      "2023-11-07 20:09:29 INFO  Job:1727 - Running job: job_1699356031669_0005\n",
      "2023-11-07 20:09:37 INFO  Job:1748 - Job job_1699356031669_0005 running in uber mode : false\n",
      "2023-11-07 20:09:37 INFO  Job:1755 -  map 0% reduce 0%\n",
      "2023-11-07 20:09:46 INFO  Job:1755 -  map 40% reduce 0%\n",
      "2023-11-07 20:09:54 INFO  Job:1755 -  map 67% reduce 0%\n",
      "2023-11-07 20:09:55 INFO  Job:1755 -  map 80% reduce 0%\n",
      "2023-11-07 20:10:01 INFO  Job:1755 -  map 100% reduce 0%\n",
      "2023-11-07 20:10:02 INFO  Job:1755 -  map 100% reduce 100%\n",
      "2023-11-07 20:10:02 INFO  Job:1766 - Job job_1699356031669_0005 completed successfully\n",
      "2023-11-07 20:10:02 INFO  Job:1773 - Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=336\n",
      "\t\tFILE: Number of bytes written=4455682\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3875\n",
      "\t\tHDFS: Number of bytes written=215\n",
      "\t\tHDFS: Number of read operations=65\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=15\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=15\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=195876\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12756\n",
      "\t\tTotal time spent by all map tasks (ms)=97938\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6378\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=97938\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6378\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=100288512\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=6531072\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=15\n",
      "\t\tMap output records=30\n",
      "\t\tMap output bytes=270\n",
      "\t\tMap output materialized bytes=420\n",
      "\t\tInput split bytes=2105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=420\n",
      "\t\tReduce input records=30\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=60\n",
      "\t\tShuffled Maps =15\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=15\n",
      "\t\tGC time elapsed (ms)=595\n",
      "\t\tCPU time spent (ms)=9940\n",
      "\t\tPhysical memory (bytes) snapshot=4057165824\n",
      "\t\tVirtual memory (bytes) snapshot=43769139200\n",
      "\t\tTotal committed heap usage (bytes)=2880438272\n",
      "\t\tPeak Map Physical memory (bytes)=266223616\n",
      "\t\tPeak Map Virtual memory (bytes)=2752069632\n",
      "\t\tPeak Reduce Physical memory (bytes)=215465984\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2740883456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1770\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=97\n",
      "Job Finished in 34.771 seconds\n",
      "Estimated value of Pi is 3.14162962962962962963\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 15 1700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fceb553-2807-490e-a11b-54fc335aa3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
